{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e84249fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk import pos_tag, RegexpParser, Tree\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b970abf",
   "metadata": {},
   "source": [
    "**Create Chunk Counter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18e8439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
    "def np_chunk_counter(chunked_sentences):\n",
    "    chunks = []\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    # a Counter object\n",
    "    chunk_counter = Counter()\n",
    "    for chunk in chunks:\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(30)\n",
    "\n",
    "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
    "def vp_chunk_counter(chunked_sentences):\n",
    "\n",
    "    #a list for chunks\n",
    "    chunks = []\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'VP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    #a Counter object\n",
    "    chunk_counter = Counter()\n",
    "    for chunk in chunks:\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f8a18",
   "metadata": {},
   "source": [
    "**Import old text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0d0da22d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import text\n",
    "old_script = open(\"E:\\Language-Chatbot\\dorian_gray.txt\", encoding='utf-8').read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad08da2c",
   "metadata": {},
   "source": [
    "**Tokenizing sentences and tokenizing words by sentences** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21f4ba15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'reveal', 'art', 'and', 'conceal', 'the', 'artist', 'is', 'art', \"'s\", 'aim', '.']\n"
     ]
    }
   ],
   "source": [
    "#word tokenized by sentences\n",
    "sentence_tokenizer = PunktSentenceTokenizer(old_script)\n",
    "sentence_tokenized = sentence_tokenizer.tokenize(old_script)\n",
    "word_tokenized = list()\n",
    "\n",
    "for tokenized_sentence in sentence_tokenized:\n",
    "    word_tokenized.append(word_tokenize(tokenized_sentence))\n",
    "print(word_tokenized[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc0877",
   "metadata": {},
   "source": [
    "**Find the most common Noun phrase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "460e4d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to', 'TO'), ('reveal', 'VB'), ('art', 'NN'), ('and', 'CC'), ('conceal', 'VB'), ('the', 'DT'), ('artist', 'NN'), ('is', 'VBZ'), ('art', 'NN'), (\"'s\", 'POS'), ('aim', 'NN'), ('.', '.')]\n",
      "(S\n",
      "  to/TO\n",
      "  reveal/VB\n",
      "  (NP art/NN)\n",
      "  and/CC\n",
      "  conceal/VB\n",
      "  (NP the/DT artist/NN)\n",
      "  is/VBZ\n",
      "  (NP art/NN)\n",
      "  's/POS\n",
      "  (NP aim/NN)\n",
      "  ./.)\n",
      "                                           S                                                 \n",
      "   ________________________________________|_____________________________________________     \n",
      "  |       |       |        |        |      |     |    NP           NP             NP     NP  \n",
      "  |       |       |        |        |      |     |    |       _____|______        |      |    \n",
      "to/TO reveal/VB and/CC conceal/VB is/VBZ 's/POS ./. art/NN the/DT     artist/NN art/NN aim/NN\n",
      "\n",
      "[((('i', 'NN'),), 963), ((('henry', 'NN'),), 200), ((('lord', 'NN'),), 197), ((('life', 'NN'),), 170), ((('harry', 'NN'),), 136), ((('dorian', 'JJ'), ('gray', 'NN')), 127), ((('something', 'NN'),), 126), ((('nothing', 'NN'),), 93), ((('basil', 'NN'),), 85), ((('the', 'DT'), ('world', 'NN')), 70), ((('everything', 'NN'),), 69), ((('anything', 'NN'),), 68), ((('hallward', 'NN'),), 68), ((('the', 'DT'), ('man', 'NN')), 61), ((('the', 'DT'), ('room', 'NN')), 60), ((('face', 'NN'),), 57), ((('the', 'DT'), ('door', 'NN')), 56), ((('love', 'NN'),), 55), ((('art', 'NN'),), 52), ((('course', 'NN'),), 51), ((('the', 'DT'), ('picture', 'NN')), 46), ((('the', 'DT'), ('lad', 'NN')), 45), ((('head', 'NN'),), 44), ((('round', 'NN'),), 44), ((('hand', 'NN'),), 44), ((('sibyl', 'NN'),), 41), ((('the', 'DT'), ('table', 'NN')), 40), ((('the', 'DT'), ('painter', 'NN')), 38), ((('sir', 'NN'),), 38), ((('a', 'DT'), ('moment', 'NN')), 38)]\n"
     ]
    }
   ],
   "source": [
    "#part-of-speech tagged sentences\n",
    "pos_tagged_text = []\n",
    "for sentence in word_tokenized:\n",
    "    pos_tagged_text.append(pos_tag(sentence))\n",
    "print(pos_tagged_text[1])\n",
    "\n",
    "#a noun phrase chunk grammar is\n",
    "np_chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)\n",
    "\n",
    "#a verb phrase chunk grammar is\n",
    "vp_chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
    "vp_chunk_parser = RegexpParser(vp_chunk_grammar)\n",
    "\n",
    "#a list for noun phrase chunked sentences \n",
    "np_chunker = []\n",
    "for sentence in pos_tagged_text:\n",
    "    np_chunker.append(np_chunk_parser.parse(sentence))\n",
    "print(np_chunker[1])\n",
    "Tree.fromstring(str(np_chunker[1])).pretty_print()\n",
    "\n",
    "#view the most occured noun phrase\n",
    "most_occured_np_chunks = np_chunk_counter(np_chunker)\n",
    "print(most_occured_np_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33252b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
