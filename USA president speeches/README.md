Word embeddings are vector representations of a word, where words with similar contexts are represented with vectors that are closer together
The distance between vectors can be calculated in many ways, and the best way for measuring the distance between higher dimensional vectors is cosine distance
Word2Vec is a shallow neural network model that can build word embeddings using either continuous bag-of-words or continuous skip-grams
Gensim is a package that allows us to create and train word embedding models using any corpus of text
