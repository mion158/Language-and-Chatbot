Vectors are containers of information, and they can have anywhere from 1-dimension to hundreds or thousands of dimensions

Word embeddings are vector representations of a word, where words with similar contexts are represented with vectors that are closer together

spaCy is a package that enables us to view and use pre-trained word embedding models

The distance between vectors can be calculated in many ways, and the best way for measuring the distance between higher dimensional vectors is cosine distance

Word2Vec is a shallow neural network model that can build word embeddings using either continuous bag-of-words or continuous skip-grams

Gensim is a package that allows us to create and train word embedding models using any corpus of text
